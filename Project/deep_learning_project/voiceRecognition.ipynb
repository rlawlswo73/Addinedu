{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프로그램 시작\n",
      "말씀해주세요...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm_dsnoop.c:601:(snd_pcm_dsnoop_open) unable to open slave\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm_dsnoop.c:601:(snd_pcm_dsnoop_open) unable to open slave\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인식된 텍스트: 나 배고파\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석 결과: {'label': 'entailment', 'score': 0.7272838354110718}\n",
      "의도: 내리고 싶다는 요청이 아닌 것으로 판단됩니다.\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from transformers import pipeline\n",
    "\n",
    "# 음성 인식 함수\n",
    "def recognize_speech():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"말씀해주세요...\")\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio, language=\"ko-KR\")  # 한국어 인식\n",
    "        print(f\"인식된 텍스트: {text}\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"음성을 이해할 수 없습니다.\")\n",
    "        return None\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"음성 인식 서비스 오류: {e}\")\n",
    "        return None\n",
    "\n",
    "# 의도 분류 함수\n",
    "def classify_intent(text):\n",
    "    # 사전 학습된 모델 로드\n",
    "    classifier = pipeline(\"text-classification\", model=\"joeddav/xlm-roberta-large-xnli\")  # 다국어 지원 모델\n",
    "    \n",
    "    # 입력 문장 분류\n",
    "    result = classifier(text)\n",
    "    return result[0]  # 가장 높은 확률의 라벨 반환\n",
    "\n",
    "# 프로그램 실행\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"프로그램 시작\")\n",
    "    user_input = recognize_speech()  # 음성 -> 텍스트\n",
    "    if user_input:\n",
    "        intent_result = classify_intent(user_input)  # 텍스트 -> 의도 분석\n",
    "        print(f\"분석 결과: {intent_result}\")\n",
    "        if \"stop\" in intent_result['label'].lower():  # 예시: 멈추는 의도 분류\n",
    "            print(\"의도: 내리고 싶다는 요청으로 판단됩니다.\")\n",
    "        else:\n",
    "            print(\"의도: 내리고 싶다는 요청이 아닌 것으로 판단됩니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 16:28:08.067286: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-26 16:28:08.106269: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-26 16:28:08.106321: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-26 16:28:08.107813: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-26 16:28:08.116193: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-26 16:28:09.180597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "녹음 시작... 말해주세요!\n",
      "녹음 완료!\n",
      "Whisper로 음성을 텍스트로 변환 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kjj37/venv/machine_learning/lib/python3.10/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환된 텍스트:  네away\n",
      "의도 분석 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석된 의도: STOP_REQUEST\n",
      "정확도: 0.44\n",
      "결론: 내리고 싶다는 요청으로 판단됩니다.\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import whisper\n",
    "import tempfile\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. 마이크로 음성 녹음\n",
    "def record_audio_from_mic(duration=5, fs=16000):\n",
    "    \"\"\"\n",
    "    마이크로 음성을 녹음하여 임시 파일로 저장합니다.\n",
    "    Args:\n",
    "        duration (int): 녹음 시간 (초).\n",
    "        fs (int): 샘플링 속도 (기본값 16000Hz).\n",
    "    Returns:\n",
    "        str: 저장된 임시 WAV 파일 경로.\n",
    "    \"\"\"\n",
    "    print(\"녹음 시작... 말해주세요!\")\n",
    "    audio_data = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "    sd.wait()  # 녹음 종료 대기\n",
    "    print(\"녹음 완료!\")\n",
    "\n",
    "    # 임시 파일 생성\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
    "    with open(temp_file.name, 'wb') as f:\n",
    "        import wave\n",
    "        wf = wave.open(f, 'wb')\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(fs)\n",
    "        wf.writeframes(audio_data.tobytes())\n",
    "        wf.close()\n",
    "\n",
    "    return temp_file.name\n",
    "\n",
    "# 2. Whisper로 음성을 텍스트로 변환\n",
    "def transcribe_audio_with_whisper(audio_path):\n",
    "    \"\"\"\n",
    "    Whisper를 사용하여 오디오 파일을 텍스트로 변환합니다.\n",
    "    Args:\n",
    "        audio_path (str): 오디오 파일 경로.\n",
    "    Returns:\n",
    "        str: 변환된 텍스트.\n",
    "    \"\"\"\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_path, language=\"ko\")\n",
    "    return result[\"text\"]\n",
    "\n",
    "# 3. Hugging Face 모델을 사용한 의도 분석\n",
    "def analyze_intent_with_accuracy(text):\n",
    "    \"\"\"\n",
    "    Hugging Face 모델을 사용하여 텍스트의 의도를 분석하고 확률을 표시합니다.\n",
    "    Args:\n",
    "        text (str): 변환된 텍스트.\n",
    "    Returns:\n",
    "        tuple: (\"STOP_REQUEST\" 또는 \"CONTINUE_REQUEST\", 정확도 점수).\n",
    "    \"\"\"\n",
    "    # 모델 로드\n",
    "    classifier = pipeline(\"text-classification\", model=\"joeddav/xlm-roberta-large-xnli\")\n",
    "    result = classifier(text)\n",
    "\n",
    "    # 결과 출력\n",
    "    label = result[0][\"label\"]\n",
    "    score = result[0][\"score\"]\n",
    "\n",
    "    # 확률과 함께 판단\n",
    "    if label == \"entailment\":\n",
    "        return \"STOP_REQUEST\", score\n",
    "    return \"CONTINUE_REQUEST\", score\n",
    "\n",
    "# 4. 통합 메인 함수\n",
    "def main():\n",
    "    duration = 5  # 녹음 시간 (초 단위)\n",
    "    audio_path = record_audio_from_mic(duration)\n",
    "\n",
    "    try:\n",
    "        print(\"Whisper로 음성을 텍스트로 변환 중...\")\n",
    "        text = transcribe_audio_with_whisper(audio_path)\n",
    "        print(f\"변환된 텍스트: {text}\")\n",
    "\n",
    "        print(\"의도 분석 중...\")\n",
    "        intent, accuracy = analyze_intent_with_accuracy(text)\n",
    "        print(f\"분석된 의도: {intent}\")\n",
    "        print(f\"정확도: {accuracy:.2f}\")  # 소수점 2자리로 정확도 출력\n",
    "\n",
    "        if intent == \"STOP_REQUEST\":\n",
    "            print(\"결론: 내리고 싶다는 요청으로 판단됩니다.\")\n",
    "        else:\n",
    "            print(\"결론: 내리고 싶지 않다는 요청으로 판단됩니다.\")\n",
    "    finally:\n",
    "        # 임시 파일 삭제\n",
    "        if os.path.exists(audio_path):\n",
    "            os.remove(audio_path)\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "택시 의도 분석 프로그램 시작!\n",
      "\n",
      "음성을 입력하려면 말하세요 (종료하려면 'exit' 입력).\n",
      "말씀하세요...\n",
      "인식된 텍스트: 아 배고프다\n",
      "분석 결과: 손님은 아직 내리기를 원하지 않습니다.\n",
      "\n",
      "음성을 입력하려면 말하세요 (종료하려면 'exit' 입력).\n",
      "말씀하세요...\n",
      "음성을 이해할 수 없습니다.\n",
      "텍스트를 인식하지 못했습니다.\n",
      "\n",
      "음성을 입력하려면 말하세요 (종료하려면 'exit' 입력).\n",
      "말씀하세요...\n",
      "인식된 텍스트: 아 심심해\n",
      "분석 결과: 손님은 아직 내리기를 원하지 않습니다.\n",
      "\n",
      "음성을 입력하려면 말하세요 (종료하려면 'exit' 입력).\n",
      "말씀하세요...\n",
      "음성을 이해할 수 없습니다.\n",
      "텍스트를 인식하지 못했습니다.\n",
      "\n",
      "음성을 입력하려면 말하세요 (종료하려면 'exit' 입력).\n",
      "말씀하세요...\n",
      "인식된 텍스트: 내리고 싶어\n",
      "분석 결과: 손님은 아직 내리기를 원하지 않습니다.\n",
      "\n",
      "음성을 입력하려면 말하세요 (종료하려면 'exit' 입력).\n",
      "말씀하세요...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m음성을 입력하려면 말하세요 (종료하려면 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m 입력).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mspeech_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mand\u001b[39;00m text\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m프로그램을 종료합니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mspeech_to_text\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     audio \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mlisten(source)\n\u001b[0;32m---> 29\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize_google\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mko-KR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m인식된 텍스트: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m~/venv/machine_learning/lib/python3.10/site-packages/speech_recognition/recognizers/google.py:256\u001b[0m, in \u001b[0;36mrecognize_legacy\u001b[0;34m(recognizer, audio_data, key, language, pfilter, show_all, with_confidence, endpoint)\u001b[0m\n\u001b[1;32m    251\u001b[0m request_builder \u001b[38;5;241m=\u001b[39m create_request_builder(\n\u001b[1;32m    252\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mendpoint, key\u001b[38;5;241m=\u001b[39mkey, language\u001b[38;5;241m=\u001b[39mlanguage, filter_level\u001b[38;5;241m=\u001b[39mpfilter\n\u001b[1;32m    253\u001b[0m )\n\u001b[1;32m    254\u001b[0m request \u001b[38;5;241m=\u001b[39m request_builder\u001b[38;5;241m.\u001b[39mbuild(audio_data)\n\u001b[0;32m--> 256\u001b[0m response_text \u001b[38;5;241m=\u001b[39m \u001b[43mobtain_transcription\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperation_timeout\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m output_parser \u001b[38;5;241m=\u001b[39m OutputParser(\n\u001b[1;32m    261\u001b[0m     show_all\u001b[38;5;241m=\u001b[39mshow_all, with_confidence\u001b[38;5;241m=\u001b[39mwith_confidence\n\u001b[1;32m    262\u001b[0m )\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_parser\u001b[38;5;241m.\u001b[39mparse(response_text)\n",
      "File \u001b[0;32m~/venv/machine_learning/lib/python3.10/site-packages/speech_recognition/recognizers/google.py:215\u001b[0m, in \u001b[0;36mobtain_transcription\u001b[0;34m(request, timeout)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobtain_transcription\u001b[39m(request: Request, timeout: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m RequestError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecognition request failed: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e\u001b[38;5;241m.\u001b[39mreason))\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:1377\u001b[0m, in \u001b[0;36mHTTPHandler.http_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:1352\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m-> 1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1354\u001b[0m     h\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1. 학습 데이터\n",
    "texts = [\n",
    "    \"여기서 내려 주세요.\", \"이쯤에서 세워 주세요.\", \"지금 멈춰 주세요.\", \"여기 괜찮아요.\", \"여기서 내릴게요.\",\n",
    "    \"잠깐만요, 여기서 내려 주세요.\", \"아, 여기 세워 주세요.\", \"저기 앞에서 멈춰 주세요.\", \"여기 내리겠습니다.\", \"여기 내려도 돼요.\",\n",
    "    \"좀 더 가주세요.\", \"다음 교차로에서 멈춰 주세요.\", \"목적지까지 가주세요.\", \"아직 내릴 곳 아니에요.\", \"조금 더 가주세요.\",\n",
    "    \"직진 부탁드립니다.\", \"좌회전해주세요.\", \"우회전 부탁드려요.\", \"목적지까지 부탁드려요.\", \"조금 더 빨리 가주세요.\"\n",
    "]\n",
    "labels = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# 데이터 벡터화\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 모델 학습\n",
    "model = LogisticRegression()\n",
    "model.fit(X, labels)\n",
    "\n",
    "# 2. 음성 인식 함수\n",
    "def speech_to_text():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"말씀하세요...\")\n",
    "        try:\n",
    "            audio = recognizer.listen(source)\n",
    "            text = recognizer.recognize_google(audio, language=\"ko-KR\")\n",
    "            print(f\"인식된 텍스트: {text}\")\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"음성을 이해할 수 없습니다.\")\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Google API 에러: {e}\")\n",
    "            return None\n",
    "\n",
    "# 3. 의도 분류 함수\n",
    "def classify_intent(text):\n",
    "    if text:\n",
    "        vectorized_text = vectorizer.transform([text])\n",
    "        prediction = model.predict(vectorized_text)\n",
    "        if prediction[0] == 1:\n",
    "            print(\"분석 결과: 손님은 여기서 내리기를 원합니다.\")\n",
    "        else:\n",
    "            print(\"분석 결과: 손님은 아직 내리기를 원하지 않습니다.\")\n",
    "    else:\n",
    "        print(\"텍스트를 인식하지 못했습니다.\")\n",
    "\n",
    "# 4. 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"택시 의도 분석 프로그램 시작!\")\n",
    "    while True:\n",
    "        print(\"\\n음성을 입력하려면 말하세요 (종료하려면 'exit' 입력).\")\n",
    "        text = speech_to_text()\n",
    "        if text and text.lower() == \"exit\":\n",
    "            print(\"프로그램을 종료합니다.\")\n",
    "            break\n",
    "        classify_intent(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import speech_recognition as sr\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1. CSV 파일에서 데이터 불러오기\n",
    "data_file = \"/mnt/data/taxi_intent_dataset_with_dialogue.csv\"\n",
    "data = pd.read_csv(data_file)\n",
    "\n",
    "texts = data[\"text\"].tolist()\n",
    "labels = data[\"label\"].tolist()\n",
    "\n",
    "# 데이터 벡터화\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 모델 학습\n",
    "model = LogisticRegression()\n",
    "model.fit(X, labels)\n",
    "\n",
    "# 2. 음성 인식 함수\n",
    "def speech_to_text():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"말씀하세요...\")\n",
    "        try:\n",
    "            audio = recognizer.listen(source)\n",
    "            text = recognizer.recognize_google(audio, language=\"ko-KR\")\n",
    "            print(f\"인식된 텍스트: {text}\")\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"음성을 이해할 수 없습니다.\")\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Google API 에러: {e}\")\n",
    "            return None\n",
    "\n",
    "# 3. 의도 분류 함수\n",
    "def classify_intent(text):\n",
    "    if text:\n",
    "        vectorized_text = vectorizer.transform([text])\n",
    "        prediction = model.predict(vectorized_text)\n",
    "        if prediction[0] == 1:\n",
    "            print(\"분석 결과: 손님은 여기서 내리기를 원합니다.\")\n",
    "        elif prediction[0] == 0:\n",
    "            print(\"분석 결과: 손님은 아직 내리기를 원하지 않습니다.\")\n",
    "        elif prediction[0] == 2:\n",
    "            print(\"분석 결과: 일상 대화로 인식되었습니다.\")\n",
    "    else:\n",
    "        print(\"텍스트를 인식하지 못했습니다.\")\n",
    "\n",
    "# 4. 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"택시 의도 분석 프로그램 시작!\")\n",
    "    while True:\n",
    "        print(\"\\n음성을 입력하려면 말하세요 (종료하려면 'exit' 입력).\")\n",
    "        text = speech_to_text()\n",
    "        if text and text.lower() == \"exit\":\n",
    "            print(\"프로그램을 종료합니다.\")\n",
    "            break\n",
    "        classify_intent(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
